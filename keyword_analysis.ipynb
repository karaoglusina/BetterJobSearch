{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Analysis: MI-Based Phrase Extraction\n",
    "\n",
    "This notebook implements the MI (Mutual Information) scoring methodology to extract meaningful domain-specific phrases from the job corpus.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Load job corpus** - Load chunks.jsonl and filter to English jobs\n",
    "2. **Extract n-grams** - Compute unigrams, bigrams, trigrams with frequencies\n",
    "3. **Compute MI scores** - Calculate PMI/NPMI for each n-gram\n",
    "4. **Compare with reference corpus** - Load Switchboard/OpenSubtitles and compute effect sizes\n",
    "5. **Filter meaningful phrases** - Keep domain-specific, cohesive phrases\n",
    "6. **Save artifacts** - Write meaningful_phrases.json for cluster labeling\n",
    "\n",
    "Based on methodology from `meta/KW extraction method/` notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Project paths\n",
    "ARTIFACTS_DIR = Path('artifacts')\n",
    "CHUNKS_PATH = ARTIFACTS_DIR / 'chunks.jsonl'\n",
    "REFERENCE_DIR = Path('meta/KW extraction method/corpora')\n",
    "\n",
    "print(f'Chunks path exists: {CHUNKS_PATH.exists()}')\n",
    "print(f'Reference dir exists: {REFERENCE_DIR.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Job Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunks\n",
    "chunks = []\n",
    "with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        chunks.append(json.loads(line))\n",
    "\n",
    "print(f'Loaded {len(chunks)} chunks')\n",
    "\n",
    "# Group by job\n",
    "jobs = {}\n",
    "for ch in chunks:\n",
    "    jk = ch.get('job_key', '')\n",
    "    if jk not in jobs:\n",
    "        meta = ch.get('meta', {})\n",
    "        jobs[jk] = {\n",
    "            'job_id': jk,\n",
    "            'title': meta.get('title', ''),\n",
    "            'company': meta.get('company', ''),\n",
    "            'language': meta.get('language', 'unknown'),\n",
    "            'texts': []\n",
    "        }\n",
    "    jobs[jk]['texts'].append(ch.get('text', ''))\n",
    "\n",
    "print(f'Unique jobs: {len(jobs)}')\n",
    "\n",
    "# Count languages\n",
    "lang_counts = Counter(j['language'] for j in jobs.values())\n",
    "print(f'Languages: {dict(lang_counts.most_common(10))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to English jobs\n",
    "english_jobs = [j for j in jobs.values() if j['language'] == 'en']\n",
    "print(f'English jobs: {len(english_jobs)} ({100*len(english_jobs)/len(jobs):.1f}%)')\n",
    "\n",
    "# Combine texts per job\n",
    "job_texts = [' '.join(j['texts']) for j in english_jobs]\n",
    "print(f'Total characters: {sum(len(t) for t in job_texts):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp.ngram_extraction import extract_ngrams_from_corpus, count_unigrams\n",
    "\n",
    "# Extract n-grams (this may take a minute)\n",
    "print('Extracting n-grams from job corpus...')\n",
    "job_ngrams = extract_ngrams_from_corpus(\n",
    "    job_texts,\n",
    "    n_min=1,\n",
    "    n_max=3,\n",
    "    min_frequency=2,\n",
    ")\n",
    "\n",
    "print(f'Extracted {len(job_ngrams)} unique n-grams')\n",
    "print(f'Unigrams: {sum(1 for ng in job_ngrams if ng[\"ngram_length\"] == 1)}')\n",
    "print(f'Bigrams: {sum(1 for ng in job_ngrams if ng[\"ngram_length\"] == 2)}')\n",
    "print(f'Trigrams: {sum(1 for ng in job_ngrams if ng[\"ngram_length\"] == 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier manipulation\n",
    "job_df = pd.DataFrame(job_ngrams)\n",
    "job_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute MI Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp.mutual_information import analyze_corpus_mi, normalize_mi_by_length\n",
    "\n",
    "# Compute MI for bigrams and trigrams\n",
    "print('Computing MI scores...')\n",
    "mi_ngrams = analyze_corpus_mi(\n",
    "    job_texts,\n",
    "    n_min=2,\n",
    "    n_max=3,\n",
    "    min_frequency=3,\n",
    ")\n",
    "\n",
    "print(f'N-grams with MI: {len(mi_ngrams)}')\n",
    "\n",
    "# Normalize MI by length\n",
    "mi_ngrams = normalize_mi_by_length(mi_ngrams)\n",
    "\n",
    "mi_df = pd.DataFrame(mi_ngrams)\n",
    "mi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top cohesive bigrams (high MI)\n",
    "bigrams = mi_df[mi_df['ngram_length'] == 2].dropna(subset=['mi_z'])\n",
    "print('Top cohesive bigrams:')\n",
    "bigrams.nlargest(20, 'mi_z')[['ngram', 'frequency', 'mi', 'mi_z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top cohesive trigrams (high MI)\n",
    "trigrams = mi_df[mi_df['ngram_length'] == 3].dropna(subset=['mi_z'])\n",
    "print('Top cohesive trigrams:')\n",
    "trigrams.nlargest(20, 'mi_z')[['ngram', 'frequency', 'mi', 'mi_z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Reference Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load Switchboard (conversational English)\n",
    "switchboard_path = REFERENCE_DIR / 'switchboard_combined.csv'\n",
    "\n",
    "if switchboard_path.exists():\n",
    "    print('Loading Switchboard corpus...')\n",
    "    ref_df = pd.read_csv(switchboard_path)\n",
    "    print(f'Switchboard shape: {ref_df.shape}')\n",
    "    print(f'Columns: {list(ref_df.columns)}')\n",
    "    \n",
    "    # Get text column (may vary by file format)\n",
    "    text_col = 'text' if 'text' in ref_df.columns else ref_df.columns[0]\n",
    "    ref_texts = ref_df[text_col].dropna().tolist()\n",
    "    print(f'Reference texts: {len(ref_texts)}')\n",
    "else:\n",
    "    print('Switchboard not found, trying OpenSubtitles...')\n",
    "    opensubtitles_path = REFERENCE_DIR / 'opensubtitles.csv'\n",
    "    if opensubtitles_path.exists():\n",
    "        ref_df = pd.read_csv(opensubtitles_path)\n",
    "        text_col = 'text' if 'text' in ref_df.columns else ref_df.columns[0]\n",
    "        ref_texts = ref_df[text_col].dropna().tolist()\n",
    "        print(f'OpenSubtitles texts: {len(ref_texts)}')\n",
    "    else:\n",
    "        print('No reference corpus found! Skipping comparison.')\n",
    "        ref_texts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract n-grams from reference corpus (if available)\n",
    "if ref_texts:\n",
    "    print('Extracting n-grams from reference corpus...')\n",
    "    # Sample for speed (use first 50k texts)\n",
    "    ref_sample = ref_texts[:50000] if len(ref_texts) > 50000 else ref_texts\n",
    "    print(f'Using {len(ref_sample)} reference texts')\n",
    "    \n",
    "    ref_ngrams = extract_ngrams_from_corpus(\n",
    "        ref_sample,\n",
    "        n_min=1,\n",
    "        n_max=3,\n",
    "        min_frequency=2,\n",
    "    )\n",
    "    print(f'Reference n-grams: {len(ref_ngrams)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare with Reference Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp.mutual_information import compare_with_reference\n",
    "\n",
    "if ref_texts:\n",
    "    # Calculate totals\n",
    "    total_job = sum(ng['frequency'] for ng in job_ngrams if ng['ngram_length'] == 1)\n",
    "    total_ref = sum(ng['frequency'] for ng in ref_ngrams if ng['ngram_length'] == 1)\n",
    "    \n",
    "    print(f'Job corpus unigrams: {total_job:,}')\n",
    "    print(f'Reference corpus unigrams: {total_ref:,}')\n",
    "    \n",
    "    # Compare n-grams\n",
    "    print('Computing effect sizes and significance...')\n",
    "    compared = compare_with_reference(\n",
    "        mi_ngrams,\n",
    "        ref_ngrams,\n",
    "        total_job,\n",
    "        total_ref,\n",
    "    )\n",
    "    \n",
    "    compared_df = pd.DataFrame(compared)\n",
    "    print(f'Compared n-grams: {len(compared_df)}')\n",
    "    compared_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top domain-specific bigrams (high effect size + significant)\n",
    "if ref_texts:\n",
    "    domain_bigrams = compared_df[\n",
    "        (compared_df['ngram_length'] == 2) &\n",
    "        (compared_df['significant'] == True) &\n",
    "        (compared_df['effect_size'] > 0)\n",
    "    ].nlargest(30, 'effect_size')\n",
    "    \n",
    "    print('Domain-specific bigrams (overrepresented in job corpus):')\n",
    "    domain_bigrams[['ngram', 'frequency', 'freq_reference', 'effect_size', 'p_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top domain-specific trigrams\n",
    "if ref_texts:\n",
    "    domain_trigrams = compared_df[\n",
    "        (compared_df['ngram_length'] == 3) &\n",
    "        (compared_df['significant'] == True) &\n",
    "        (compared_df['effect_size'] > 0)\n",
    "    ].nlargest(30, 'effect_size')\n",
    "    \n",
    "    print('Domain-specific trigrams:')\n",
    "    domain_trigrams[['ngram', 'frequency', 'freq_reference', 'effect_size', 'p_value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Filter Meaningful Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp.mutual_information import filter_meaningful_phrases\n",
    "\n",
    "# Filter to meaningful phrases\n",
    "if ref_texts:\n",
    "    meaningful = filter_meaningful_phrases(\n",
    "        compared,\n",
    "        min_mi_z=0.0,\n",
    "        min_effect_size=0.0,\n",
    "        require_significant=True,\n",
    "        min_frequency=3,\n",
    "        min_doc_frequency=2,\n",
    "    )\n",
    "else:\n",
    "    # Without reference, use MI alone\n",
    "    meaningful = [\n",
    "        ng for ng in mi_ngrams\n",
    "        if ng.get('mi_z', 0) > 0.5\n",
    "        and ng.get('frequency', 0) >= 3\n",
    "        and ng.get('ngram_length', 1) >= 2\n",
    "    ]\n",
    "\n",
    "print(f'Meaningful phrases: {len(meaningful)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sample of meaningful phrases\n",
    "meaningful_df = pd.DataFrame(meaningful)\n",
    "if 'effect_size' in meaningful_df.columns:\n",
    "    # Sort by combined score (MI + effect size)\n",
    "    meaningful_df['score'] = meaningful_df['mi_z'].fillna(0) + meaningful_df['effect_size'].fillna(0)\n",
    "    meaningful_df.nlargest(50, 'score')[['ngram', 'frequency', 'mi_z', 'effect_size', 'score']]\n",
    "else:\n",
    "    meaningful_df.nlargest(50, 'mi_z')[['ngram', 'frequency', 'mi_z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output\n",
    "output = {\n",
    "    'phrases': [ng['ngram'] for ng in meaningful],\n",
    "    'n_phrases': len(meaningful),\n",
    "    'n_jobs_analyzed': len(english_jobs),\n",
    "    'details': meaningful[:1000],  # Save top 1000 with full details\n",
    "}\n",
    "\n",
    "# Save to artifacts\n",
    "output_path = ARTIFACTS_DIR / 'meaningful_phrases.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f'Saved {len(meaningful)} phrases to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save full analysis as parquet for future use\n",
    "full_df = meaningful_df if 'meaningful_df' in dir() else pd.DataFrame(meaningful)\n",
    "parquet_path = ARTIFACTS_DIR / 'phrase_scores.parquet'\n",
    "full_df.to_parquet(parquet_path, index=False)\n",
    "print(f'Saved full analysis to {parquet_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The analysis has:\n",
    "1. Loaded the job corpus and filtered to English jobs\n",
    "2. Extracted n-grams and computed MI scores\n",
    "3. Compared with reference corpus to find domain-specific phrases\n",
    "4. Saved meaningful phrases to `artifacts/meaningful_phrases.json`\n",
    "\n",
    "These phrases can now be used in cluster labeling to filter out irrelevant TF-IDF keywords."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
